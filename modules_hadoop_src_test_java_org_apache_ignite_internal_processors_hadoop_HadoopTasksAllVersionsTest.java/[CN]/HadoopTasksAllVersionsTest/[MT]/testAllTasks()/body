{
  IgfsPath inDir=new IgfsPath(PATH_INPUT);
  igfs.mkdirs(inDir);
  IgfsPath inFile=new IgfsPath(inDir,HadoopWordCount2.class.getSimpleName() + "-input");
  URI inFileUri=URI.create(igfsScheme() + inFile.toString());
  generateTestFile(inFile.toString(),"red",100,"blue",200,"green",150,"yellow",70);
  long fileLen=igfs.info(inFile).length();
  Long l=fileLen / 2;
  HadoopFileBlock fileBlock1=new HadoopFileBlock(HOSTS,inFileUri,0,l);
  HadoopFileBlock fileBlock2=new HadoopFileBlock(HOSTS,inFileUri,l,fileLen - l);
  HadoopJob gridJob=getHadoopJob(inFileUri.toString(),igfsScheme() + PATH_OUTPUT);
  HadoopTestTaskContext combine1Ctx=runMapCombineTask(fileBlock1,gridJob);
  HadoopTestTaskContext combine2Ctx=runMapCombineTask(fileBlock2,gridJob);
  HadoopTaskInfo taskInfo=new HadoopTaskInfo(HadoopTaskType.REDUCE,gridJob.id(),0,0,null);
  HadoopTestTaskContext reduceCtx=new HadoopTestTaskContext(taskInfo,gridJob);
  reduceCtx.makeTreeOfWritables(combine1Ctx.mockOutput());
  reduceCtx.makeTreeOfWritables(combine2Ctx.mockOutput());
  reduceCtx.run();
  reduceCtx.taskInfo(new HadoopTaskInfo(HadoopTaskType.COMMIT,gridJob.id(),0,0,null));
  reduceCtx.run();
  assertEquals("blue\t200\n" + "green\t150\n" + "red\t100\n"+ "yellow\t70\n",readAndSortFile(PATH_OUTPUT + "/" + getOutputFileNamePrefix()+ "00000"));
}
