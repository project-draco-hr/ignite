{
  IgfsPath inDir=new IgfsPath(PATH_INPUT);
  igfs.mkdirs(inDir);
  IgfsPath inFile=new IgfsPath(inDir,HadoopWordCount2.class.getSimpleName() + "-input");
  generateTestFile(inFile.toString(),"key1",10000,"key2",20000,"key3",15000,"key4",7000,"key5",12000,"key6",18000);
  for (int i=0; i < 2; i++) {
    boolean useNewAPI=i == 1;
    igfs.delete(new IgfsPath(PATH_OUTPUT),true);
    flags.put("serializationWasConfigured",false);
    flags.put("partitionerWasConfigured",false);
    flags.put("inputFormatWasConfigured",false);
    flags.put("outputFormatWasConfigured",false);
    JobConf jobConf=new JobConf();
    jobConf.set(CommonConfigurationKeys.IO_SERIALIZATIONS_KEY,CustomSerialization.class.getName());
    jobConf.setInt(FileInputFormat.SPLIT_MAXSIZE,65000);
    jobConf.setInt("fs.local.block.size",65000);
    setupFileSystems(jobConf);
    HadoopWordCount1.setTasksClasses(jobConf,!useNewAPI,!useNewAPI,!useNewAPI);
    if (!useNewAPI) {
      jobConf.setPartitionerClass(CustomV1Partitioner.class);
      jobConf.setInputFormat(CustomV1InputFormat.class);
      jobConf.setOutputFormat(CustomV1OutputFormat.class);
    }
    Job job=Job.getInstance(jobConf);
    HadoopWordCount2.setTasksClasses(job,useNewAPI,useNewAPI,useNewAPI,false);
    if (useNewAPI) {
      job.setPartitionerClass(CustomV2Partitioner.class);
      job.setInputFormatClass(CustomV2InputFormat.class);
      job.setOutputFormatClass(CustomV2OutputFormat.class);
    }
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(IntWritable.class);
    FileInputFormat.setInputPaths(job,new Path(igfsScheme() + inFile.toString()));
    FileOutputFormat.setOutputPath(job,new Path(igfsScheme() + PATH_OUTPUT));
    job.setNumReduceTasks(3);
    job.setJarByClass(HadoopWordCount2.class);
    IgniteInternalFuture<?> fut=grid(0).hadoop().submit(new HadoopJobId(UUID.randomUUID(),1),createJobInfo(job.getConfiguration()));
    fut.get();
    assertTrue("Serialization was configured (new API is " + useNewAPI + ")",flags.get("serializationWasConfigured"));
    assertTrue("Partitioner was configured (new API is = " + useNewAPI + ")",flags.get("partitionerWasConfigured"));
    assertTrue("Input format was configured (new API is = " + useNewAPI + ")",flags.get("inputFormatWasConfigured"));
    assertTrue("Output format was configured (new API is = " + useNewAPI + ")",flags.get("outputFormatWasConfigured"));
    assertEquals("Use new API = " + useNewAPI,"key3\t15000\n" + "key6\t18000\n",readAndSortFile(PATH_OUTPUT + "/" + (useNewAPI ? "part-r-" : "part-")+ "00000"));
    assertEquals("Use new API = " + useNewAPI,"key1\t10000\n" + "key4\t7000\n",readAndSortFile(PATH_OUTPUT + "/" + (useNewAPI ? "part-r-" : "part-")+ "00001"));
    assertEquals("Use new API = " + useNewAPI,"key2\t20000\n" + "key5\t12000\n",readAndSortFile(PATH_OUTPUT + "/" + (useNewAPI ? "part-r-" : "part-")+ "00002"));
  }
}
