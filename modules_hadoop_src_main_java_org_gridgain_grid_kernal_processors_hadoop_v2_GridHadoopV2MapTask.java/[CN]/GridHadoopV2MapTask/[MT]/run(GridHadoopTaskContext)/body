{
  GridHadoopV2Job jobImpl=(GridHadoopV2Job)taskCtx.job();
  JobContext jobCtx=jobImpl.hadoopJobContext();
  Mapper mapper;
  InputFormat inFormat;
  try {
    mapper=U.newInstance(jobCtx.getMapperClass());
    inFormat=U.newInstance(jobCtx.getInputFormatClass());
  }
 catch (  ClassNotFoundException e) {
    throw new GridException(e);
  }
  GridHadoopV2Context hadoopCtx=new GridHadoopV2Context(jobCtx.getConfiguration(),taskCtx,jobImpl.attemptId(info()));
  GridHadoopInputSplit split=info().inputSplit();
  InputSplit nativeSplit;
  if (split instanceof GridHadoopFileBlock) {
    GridHadoopFileBlock block=(GridHadoopFileBlock)split;
    nativeSplit=new FileSplit(new Path(block.file().toString()),block.start(),block.length(),block.hosts());
  }
 else   nativeSplit=(InputSplit)split.innerSplit();
  try {
    RecordReader reader=inFormat.createRecordReader(nativeSplit,hadoopCtx);
    reader.initialize(nativeSplit,hadoopCtx);
    hadoopCtx.reader(reader);
    mapper.run(new WrappedMapper().getMapContext(hadoopCtx));
  }
 catch (  IOException e) {
    throw new GridException(e);
  }
catch (  InterruptedException e) {
    Thread.currentThread().interrupt();
    throw new GridInterruptedException(e);
  }
}
