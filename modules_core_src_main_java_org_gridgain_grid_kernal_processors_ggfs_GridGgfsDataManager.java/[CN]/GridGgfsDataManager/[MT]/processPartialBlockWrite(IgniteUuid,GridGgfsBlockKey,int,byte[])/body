{
  if (dataCachePrj.ggfsDataSpaceUsed() >= dataCachePrj.ggfsDataSpaceMax()) {
    try {
      ggfs.awaitDeletesAsync().get(trashPurgeTimeout);
    }
 catch (    IgniteFutureTimeoutException ignore) {
    }
    if (dataCachePrj.ggfsDataSpaceUsed() >= dataCachePrj.ggfsDataSpaceMax()) {
      final WriteCompletionFuture completionFut=pendingWrites.get(fileId);
      if (completionFut == null) {
        if (log.isDebugEnabled())         log.debug("Missing completion future for file write request (most likely exception occurred " + "which will be thrown upon stream close) [fileId=" + fileId + ']');
        return;
      }
      completionFut.onLocalError(new IgniteFsOutOfSpaceException("Failed to write data block " + "(GGFS maximum data size exceeded) [used=" + dataCachePrj.ggfsDataSpaceUsed() + ", allowed="+ dataCachePrj.ggfsDataSpaceMax()+ ']'));
      return;
    }
  }
  if (colocatedKey.affinityKey() == null) {
    dataCachePrj.transform(colocatedKey,new UpdateClosure(startOff,data));
    return;
  }
  if (startOff == 0) {
    dataCachePrj.putx(colocatedKey,data);
    return;
  }
  GridGgfsBlockKey key=new GridGgfsBlockKey(colocatedKey.getFileId(),null,colocatedKey.evictExclude(),colocatedKey.getBlockId());
  GridCacheTx tx=dataCachePrj.txStart(PESSIMISTIC,REPEATABLE_READ);
  try {
    Map<GridGgfsBlockKey,byte[]> vals=dataCachePrj.getAll(F.asList(colocatedKey,key));
    boolean hasVal=false;
    UpdateClosure transformClos=new UpdateClosure(startOff,data);
    if (vals.get(colocatedKey) != null) {
      dataCachePrj.transform(colocatedKey,transformClos);
      hasVal=true;
    }
    if (vals.get(key) != null) {
      dataCachePrj.transform(key,transformClos);
      hasVal=true;
    }
    if (!hasVal)     throw new IgniteCheckedException("Failed to write partial block (no previous data was found in cache) " + "[key=" + colocatedKey + ", relaxedKey="+ key+ ", startOff="+ startOff+ ", dataLen="+ data.length+ ']');
    tx.commit();
  }
  finally {
    tx.close();
  }
}
