{
  igfs.delete(new IgfsPath(PATH_OUTPUT),true);
  JobConf jobConf=new JobConf();
  jobConf.set(JOB_COUNTER_WRITER_PROPERTY,IgniteHadoopFileSystemCounterWriter.class.getName());
  jobConf.setUser(USER);
  jobConf.set(IgniteHadoopFileSystemCounterWriter.COUNTER_WRITER_DIR_PROPERTY,"/xxx/${USER}/zzz");
  jobConf.setInt(FileInputFormat.SPLIT_MAXSIZE,65000);
  jobConf.setInt("fs.local.block.size",65000);
  setupFileSystems(jobConf);
  HadoopWordCount1.setTasksClasses(jobConf,!useNewMapper,!useNewCombiner,!useNewReducer);
  Job job=Job.getInstance(jobConf);
  HadoopWordCount2.setTasksClasses(job,useNewMapper,useNewCombiner,useNewReducer,compressOutputSnappy());
  job.setOutputKeyClass(Text.class);
  job.setOutputValueClass(IntWritable.class);
  FileInputFormat.setInputPaths(job,new Path(igfsScheme() + inFile.toString()));
  FileOutputFormat.setOutputPath(job,new Path(igfsScheme() + PATH_OUTPUT));
  job.setJarByClass(HadoopWordCount2.class);
  HadoopJobId jobId=new HadoopJobId(UUID.randomUUID(),1);
  IgniteInternalFuture<?> fut=grid(0).hadoop().submit(jobId,createJobInfo(job.getConfiguration()));
  fut.get();
  checkJobStatistics(jobId);
  final String outFile=PATH_OUTPUT + "/" + (useNewReducer ? "part-r-" : "part-")+ "00000";
  checkOwner(new IgfsPath(PATH_OUTPUT + "/" + "_SUCCESS"));
  checkOwner(new IgfsPath(outFile));
  String actual=readAndSortFile(outFile,job.getConfiguration());
  assertEquals("Use new mapper: " + useNewMapper + ", new combiner: "+ useNewCombiner+ ", new reducer: "+ useNewReducer,"blue\t" + blue + "\n"+ "green\t"+ green+ "\n"+ "red\t"+ red+ "\n"+ "yellow\t"+ yellow+ "\n",actual);
}
