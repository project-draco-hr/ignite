{
  GridHadoopV2Job jobImpl=(GridHadoopV2Job)taskCtx.job();
  JobConf jobConf=new JobConf(jobImpl.hadoopJobContext().getJobConf());
  Mapper mapper=U.newInstance(jobConf.getMapperClass());
  InputFormat inFormat=jobConf.getInputFormat();
  GridHadoopInputSplit split=info().inputSplit();
  InputSplit nativeSplit;
  if (split instanceof GridHadoopFileBlock) {
    GridHadoopFileBlock block=(GridHadoopFileBlock)split;
    nativeSplit=new FileSplit(new Path(block.file().toString()),block.start(),block.length(),block.hosts());
  }
 else   nativeSplit=(InputSplit)split.innerSplit();
  OutputCollector collector=new OutputCollector(){
    @Override public void collect(    Object key,    Object val) throws IOException {
      try {
        taskCtx.output().write(key,val);
      }
 catch (      GridException e) {
        throw new IOException(e);
      }
    }
  }
;
  Reporter reporter=Reporter.NULL;
  try {
    RecordReader reader=inFormat.getRecordReader(nativeSplit,jobConf,reporter);
    Object key=reader.createKey();
    Object val=reader.createValue();
    mapper.configure(jobConf);
    while (reader.next(key,val))     mapper.map(key,val,collector,reporter);
    mapper.close();
  }
 catch (  IOException e) {
    throw new GridException(e);
  }
}
