{
  GridHadoopV2Job job=(GridHadoopV2Job)taskCtx.job();
  JobConf jobConf=job.hadoopJobContext().getJobConf();
  Reducer reducer=U.newInstance(jobConf.getReducerClass());
  OutputFormat outFormat=jobConf.getOutputFormat();
  Reporter reporter=Reporter.NULL;
  NumberFormat numFormat=NumberFormat.getInstance();
  numFormat.setMinimumIntegerDigits(5);
  numFormat.setGroupingUsed(false);
  String fileName="part-" + numFormat.format(info().taskNumber());
  reducer.configure(jobConf);
  GridHadoopTaskInput input=taskCtx.input();
  TaskAttemptID attempt=job.attemptId(info());
  jobConf.set("mapreduce.task.attempt.id",attempt.toString());
  try {
    final RecordWriter writer=outFormat.getRecordWriter(null,jobConf,fileName,reporter);
    OutputCollector collector=new OutputCollector(){
      @Override public void collect(      Object key,      Object val) throws IOException {
        writer.write(key,val);
      }
    }
;
    try {
      while (input.next())       reducer.reduce(input.key(),input.values(),collector,reporter);
      reducer.close();
    }
  finally {
      writer.close(reporter);
    }
    OutputCommitter commiter=jobConf.getOutputCommitter();
    commiter.commitTask(new TaskAttemptContextImpl(jobConf,attempt));
  }
 catch (  IOException e) {
    throw new GridException(e);
  }
}
