{
  enterBusy();
  try {
    if (rmtClient != null)     throw new IOException("File system is already initialized: " + rmtClient);
    A.notNull(name,"name");
    A.notNull(cfg,"cfg");
    super.initialize(name,cfg);
    setConf(cfg);
    mgmt=cfg.getBoolean(IGFS_MANAGEMENT,false);
    if (!IGFS_SCHEME.equals(name.getScheme()))     throw new IOException("Illegal file system URI [expected=" + IGFS_SCHEME + "://[name]/[optional_path], actual="+ name+ ']');
    uri=name;
    uriAuthority=uri.getAuthority();
    user=getFsHadoopUser();
    seqReadsBeforePrefetch=parameter(cfg,PARAM_IGFS_SEQ_READS_BEFORE_PREFETCH,uriAuthority,0);
    if (seqReadsBeforePrefetch > 0)     seqReadsBeforePrefetchOverride=true;
    dfltReplication=(short)cfg.getInt("dfs.replication",3);
    colocateFileWrites=parameter(cfg,PARAM_IGFS_COLOCATED_WRITES,uriAuthority,false);
    preferLocFileWrites=cfg.getBoolean(PARAM_IGFS_PREFER_LOCAL_WRITES,false);
    String logDirCfg=parameter(cfg,PARAM_IGFS_LOG_DIR,uriAuthority,DFLT_IGFS_LOG_DIR);
    File logDirFile=U.resolveIgnitePath(logDirCfg);
    String logDir=logDirFile != null ? logDirFile.getAbsolutePath() : null;
    rmtClient=new HadoopIgfsWrapper(uriAuthority,logDir,cfg,LOG,user);
    IgfsHandshakeResponse handshake=rmtClient.handshake(logDir);
    igfsGrpBlockSize=handshake.blockSize();
    IgfsPaths paths=handshake.secondaryPaths();
    Boolean logEnabled=parameter(cfg,PARAM_IGFS_LOG_ENABLED,uriAuthority,false);
    if (handshake.sampling() != null ? handshake.sampling() : logEnabled) {
      if (logDir == null)       throw new IOException("Failed to resolve log directory: " + logDirCfg);
      Integer batchSize=parameter(cfg,PARAM_IGFS_LOG_BATCH_SIZE,uriAuthority,DFLT_IGFS_LOG_BATCH_SIZE);
      clientLog=IgfsLogger.logger(uriAuthority,handshake.igfsName(),logDir,batchSize);
    }
 else     clientLog=IgfsLogger.disabledLogger();
    try {
      modeRslvr=new IgfsModeResolver(paths.defaultMode(),paths.pathModes());
    }
 catch (    IgniteCheckedException ice) {
      throw new IOException(ice);
    }
    boolean initSecondary=paths.defaultMode() == PROXY;
    if (!initSecondary && paths.pathModes() != null && !paths.pathModes().isEmpty()) {
      for (      T2<IgfsPath,IgfsMode> pathMode : paths.pathModes()) {
        IgfsMode mode=pathMode.getValue();
        if (mode == PROXY) {
          initSecondary=true;
          break;
        }
      }
    }
    if (initSecondary) {
      try {
        HadoopFileSystemFactory factory0=(HadoopFileSystemFactory)paths.getPayload(getClass().getClassLoader());
        factory=HadoopDelegateUtils.fileSystemFactoryDelegate(factory0);
      }
 catch (      IgniteCheckedException e) {
        throw new IOException("Failed to get secondary file system factory.",e);
      }
      if (factory == null)       throw new IOException("Failed to get secondary file system factory (did you set " + IgniteHadoopIgfsSecondaryFileSystem.class.getName() + " as \"secondaryFIleSystem\" in "+ FileSystemConfiguration.class.getName()+ "?)");
      factory.start();
      try {
        FileSystem secFs=(FileSystem)factory.get(user);
        secondaryUri=secFs.getUri();
        A.ensure(secondaryUri != null,"Secondary file system uri should not be null.");
      }
 catch (      IOException e) {
        if (!mgmt)         throw new IOException("Failed to connect to the secondary file system: " + secondaryUri,e);
 else         LOG.warn("Visor failed to create secondary file system (operations on paths with PROXY mode " + "will have no effect): " + e.getMessage());
      }
    }
    setWorkingDirectory(null);
  }
  finally {
    leaveBusy();
  }
}
