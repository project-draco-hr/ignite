{
  try {
    GridKernal kernal=(GridKernal)grid(0);
    GridHadoopProcessorAdapter hadoop=kernal.context().hadoop();
    UUID globalId=UUID.randomUUID();
    Configuration cfg=new Configuration();
    int cnt=10;
    cfg.setInt(BLOCK_CNT,cnt);
    cfg.setClass(MRJobConfig.COMBINE_CLASS_ATTR,TestCombiner.class,Reducer.class);
    GridHadoopJobId jobId=new GridHadoopJobId(globalId,1);
    hadoop.submit(jobId,new GridHadoopDefaultJobInfo(cfg));
    checkStatus(jobId,false);
    info("Releasing map latch.");
    mapAwaitLatch.countDown();
    checkStatus(jobId,false);
    U.sleep(50);
    for (int i=0; i < gridCount(); i++) {
      Grid g=grid(i);
      UUID nodeId=g.localNode().id();
      assertEquals(0,reduceExecCnt.get(nodeId).get());
    }
    info("Releasing combiner latch.");
    combineAwaitLatch.countDown();
    checkStatus(jobId,false);
    info("Releasing reduce latch.");
    reduceAwaitLatch.countDown();
    checkStatus(jobId,true);
    int maps=0;
    int reduces=0;
    int combines=0;
    for (int i=0; i < gridCount(); i++) {
      Grid g=grid(i);
      UUID nodeId=g.localNode().id();
      maps+=mapExecCnt.get(nodeId).get();
      combines+=combineExecCnt.get(nodeId).get();
      reduces+=reduceExecCnt.get(nodeId).get();
    }
    assertEquals(10,maps);
    assertEquals(3,combines);
    assertEquals(1,reduces);
  }
  finally {
    mapAwaitLatch.countDown();
    combineAwaitLatch.countDown();
    reduceAwaitLatch.countDown();
  }
}
