{
  IgfsPath inDir=new IgfsPath(PATH_INPUT);
  igfs.mkdirs(inDir);
  IgfsPath inFile=new IgfsPath(inDir,GridHadoopWordCount2.class.getSimpleName() + "-input");
  generateTestFile(inFile.toString(),"red",100000,"blue",200000,"green",150000,"yellow",70000);
  for (int i=0; i < 8; i++) {
    igfs.delete(new IgfsPath(PATH_OUTPUT),true);
    boolean useNewMapper=(i & 1) == 0;
    boolean useNewCombiner=(i & 2) == 0;
    boolean useNewReducer=(i & 4) == 0;
    JobConf jobConf=new JobConf();
    jobConf.set(JOB_COUNTER_WRITER_PROPERTY,GridHadoopFSCounterWriter.class.getName());
    jobConf.setUser("yyy");
    jobConf.set(GridHadoopFSCounterWriter.COUNTER_WRITER_DIR_PROPERTY,"/xxx/${USER}/zzz");
    jobConf.setInt(FileInputFormat.SPLIT_MAXSIZE,65000);
    jobConf.setInt("fs.local.block.size",65000);
    setupFileSystems(jobConf);
    GridHadoopWordCount1.setTasksClasses(jobConf,!useNewMapper,!useNewCombiner,!useNewReducer);
    Job job=Job.getInstance(jobConf);
    GridHadoopWordCount2.setTasksClasses(job,useNewMapper,useNewCombiner,useNewReducer);
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(IntWritable.class);
    FileInputFormat.setInputPaths(job,new Path(igfsScheme() + inFile.toString()));
    FileOutputFormat.setOutputPath(job,new Path(igfsScheme() + PATH_OUTPUT));
    job.setJarByClass(GridHadoopWordCount2.class);
    GridHadoopJobId jobId=new GridHadoopJobId(UUID.randomUUID(),1);
    IgniteInternalFuture<?> fut=grid(0).hadoop().submit(jobId,createJobInfo(job.getConfiguration()));
    fut.get();
    checkJobStatistics(jobId);
    assertEquals("Use new mapper: " + useNewMapper + ", new combiner: "+ useNewCombiner+ ", new reducer: "+ useNewReducer,"blue\t200000\n" + "green\t150000\n" + "red\t100000\n"+ "yellow\t70000\n",readAndSortFile(PATH_OUTPUT + "/" + (useNewReducer ? "part-r-" : "part-")+ "00000"));
  }
}
