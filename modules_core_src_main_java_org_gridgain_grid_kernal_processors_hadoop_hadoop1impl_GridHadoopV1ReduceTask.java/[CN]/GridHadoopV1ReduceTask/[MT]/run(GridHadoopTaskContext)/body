{
  GridHadoopV1JobImpl jobImpl=(GridHadoopV1JobImpl)taskCtx.job();
  JobContext jobCtx=jobImpl.hadoopJobContext();
  Reducer reducer=U.newInstance(jobCtx.getJobConf().getReducerClass());
  OutputFormat outFormat=jobCtx.getJobConf().getOutputFormat();
  Reporter reporter=Reporter.NULL;
  NumberFormat numberFormat=NumberFormat.getInstance();
  numberFormat.setMinimumIntegerDigits(5);
  numberFormat.setGroupingUsed(false);
  String fileName="part-" + numberFormat.format(info().taskNumber());
  reducer.configure(jobCtx.getJobConf());
  GridHadoopTaskInput input=taskCtx.input();
  TaskAttemptID attempt=jobImpl.attemptId(info());
  jobCtx.getJobConf().set("mapreduce.task.attempt.id",attempt.toString());
  try {
    final RecordWriter writer=outFormat.getRecordWriter(null,jobCtx.getJobConf(),fileName,reporter);
    OutputCollector collector=new OutputCollector(){
      @Override public void collect(      Object key,      Object val) throws IOException {
        writer.write(key,val);
      }
    }
;
    try {
      while (input.next()) {
        reducer.reduce(input.key(),input.values(),collector,reporter);
      }
      reducer.close();
    }
  finally {
      writer.close(reporter);
    }
    OutputCommitter commiter=jobCtx.getJobConf().getOutputCommitter();
    commiter.commitTask(new TaskAttemptContextImpl(jobCtx.getJobConf(),attempt));
  }
 catch (  IOException e) {
    throw new GridException(e);
  }
}
