{
  if (log.isDebugEnabled())   log.debug("Processing discovery event [locNodeId=" + ctx.localNodeId() + ", evt="+ evt+ ']');
  boolean checkSetup=evt.eventNode().order() < ctx.localNodeOrder();
  if (ctx.jobUpdateLeader()) {
    for (    GridHadoopJobMetadata meta : jobMetaPrj.values()) {
      GridHadoopJobId jobId=meta.jobId();
      GridHadoopMapReducePlan plan=meta.mapReducePlan();
      GridHadoopJobPhase phase=meta.phase();
      try {
        if (checkSetup && phase == PHASE_SETUP && !activeJobs.containsKey(jobId)) {
          GridHadoopJob job=ctx.jobFactory().createJob(jobId,meta.jobInfo());
          ctx.taskExecutor().run(job,setupTask(job,meta));
        }
 else         if (phase == PHASE_MAP || phase == PHASE_REDUCE) {
          Collection<GridHadoopInputSplit> cancelSplits=null;
          for (          UUID nodeId : plan.mapperNodeIds()) {
            if (ctx.kernalContext().discovery().node(nodeId) == null) {
              Collection<GridHadoopInputSplit> mappers=plan.mappers(nodeId);
              if (cancelSplits == null)               cancelSplits=new HashSet<>();
              cancelSplits.addAll(mappers);
            }
          }
          Collection<Integer> cancelReducers=null;
          for (          UUID nodeId : plan.reducerNodeIds()) {
            if (ctx.kernalContext().discovery().node(nodeId) == null) {
              int[] reducers=plan.reducers(nodeId);
              if (cancelReducers == null)               cancelReducers=new HashSet<>();
              for (              int rdc : reducers)               cancelReducers.add(rdc);
            }
          }
          if (cancelSplits != null || cancelReducers != null)           jobMetaPrj.transform(meta.jobId(),new CancelJobClosure(new GridException("One or " + "more nodes participating in map-reduce job execution failed."),cancelSplits,cancelReducers));
        }
      }
 catch (      GridException e) {
        U.error(log,"Failed to cancel job: " + meta,e);
      }
    }
  }
}
