{
  Thread.currentThread().setContextClassLoader(jobConf.getClassLoader());
  try {
    String jobDirPath=jobConf.get(MRJobConfig.MAPREDUCE_JOB_DIR);
    if (jobDirPath == null) {
      if (jobConf.getUseNewMapper())       return GridHadoopV2Splitter.splitJob(jobCtx);
 else       return GridHadoopV1Splitter.splitJob(jobConf);
    }
    Path jobDir=new Path(jobDirPath);
    try (FileSystem fs=FileSystem.get(jobDir.toUri(),jobConf)){
      JobSplit.TaskSplitMetaInfo[] metaInfos=SplitMetaInfoReader.readSplitMetaInfo(hadoopJobID,fs,jobConf,jobDir);
      if (F.isEmpty(metaInfos))       throw new GridException("No input splits found.");
      Path splitsFile=JobSubmissionFiles.getJobSplitFile(jobDir);
      try (FSDataInputStream in=fs.open(splitsFile)){
        Collection<GridHadoopInputSplit> res=new ArrayList<>(metaInfos.length);
        for (        JobSplit.TaskSplitMetaInfo metaInfo : metaInfos) {
          long off=metaInfo.getStartOffset();
          String[] hosts=metaInfo.getLocations();
          in.seek(off);
          String clsName=Text.readString(in);
          GridHadoopFileBlock block=GridHadoopV1Splitter.readFileBlock(clsName,in,hosts);
          if (block == null)           block=GridHadoopV2Splitter.readFileBlock(clsName,in,hosts);
          res.add(block != null ? block : new GridHadoopExternalSplit(hosts,off));
        }
        return res;
      }
     }
 catch (    Throwable e) {
      throw transformException(e);
    }
  }
  finally {
    Thread.currentThread().setContextClassLoader(null);
  }
}
