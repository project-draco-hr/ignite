{
  JobConf cfg=ctx.getJobConf();
  String jobDirPath=cfg.get(MRJobConfig.MAPREDUCE_JOB_DIR);
  if (jobDirPath == null) {
    if (useNewMapper)     return GridHadoopV2Splitter.splitJob(ctx);
 else     return GridHadoopV1Splitter.splitJob(cfg);
  }
  Path jobDir=new Path(jobDirPath);
  try (FileSystem fs=FileSystem.get(jobDir.toUri(),cfg)){
    JobSplit.TaskSplitMetaInfo[] metaInfos=SplitMetaInfoReader.readSplitMetaInfo(hadoopJobID,fs,cfg,jobDir);
    if (F.isEmpty(metaInfos))     throw new GridException("No input splits found.");
    Path splitsFile=JobSubmissionFiles.getJobSplitFile(jobDir);
    try (FSDataInputStream in=fs.open(splitsFile)){
      Collection<GridHadoopInputSplit> res=new ArrayList<>(metaInfos.length);
      for (      JobSplit.TaskSplitMetaInfo metaInfo : metaInfos) {
        long off=metaInfo.getStartOffset();
        String[] hosts=metaInfo.getLocations();
        Class<?> cls=readSplitClass(in,off);
        GridHadoopFileBlock block=null;
        if (cls != null) {
          block=GridHadoopV1Splitter.readFileBlock(cls,in,hosts);
          if (block == null)           block=GridHadoopV2Splitter.readFileBlock(cls,in,hosts);
        }
        res.add(block != null ? block : new GridHadoopExternalSplit(hosts,off));
      }
      return res;
    }
   }
 catch (  IOException e) {
    throw new GridException(e);
  }
}
