{
  File testInputFile=File.createTempFile(GridHadoopWordCount2.class.getSimpleName(),"-input");
  testInputFile.deleteOnExit();
  generateTestFile(testInputFile,"red",100000,"blue",200000,"green",150000,"yellow",70000);
  File testOutputDir=Files.createTempDirectory("job-output").toFile();
  for (int i=0; i < 16; i++) {
    boolean useNewMapper=(i & 1) == 0;
    boolean useNewCombiner=(i & 2) == 0;
    boolean useNewReducer=(i & 4) == 0;
    boolean useCustomSerializer=(i & 8) == 0;
    JobConf jobConf=new JobConf();
    if (useCustomSerializer)     jobConf.set(CommonConfigurationKeys.IO_SERIALIZATIONS_KEY,CustomSerialization.class.getName());
    jobConf.setInt(FileInputFormat.SPLIT_MAXSIZE,65000);
    GridHadoopWordCount1.setTasksClasses(jobConf,!useNewMapper,!useNewCombiner,!useNewReducer);
    Job job=Job.getInstance(jobConf);
    GridHadoopWordCount2.setTasksClasses(job,useNewMapper,useNewCombiner,useNewReducer);
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(IntWritable.class);
    FileInputFormat.setInputPaths(job,new Path(testInputFile.getAbsolutePath()));
    FileOutputFormat.setOutputPath(job,new Path(testOutputDir.getAbsolutePath()));
    job.setJarByClass(GridHadoopWordCount2.class);
    try {
      GridHadoopProcessorAdapter hadoop=((GridKernal)grid(0)).context().hadoop();
      GridFuture<?> fut=hadoop.submit(new GridHadoopJobId(UUID.randomUUID(),1),new GridHadoopDefaultJobInfo(job.getConfiguration()));
      fut.get();
      assertEquals("Use new mapper = " + useNewMapper + ", combiner = "+ useNewCombiner+ "reducer = "+ useNewReducer,"green\t150000\n" + "blue\t200000\n" + "red\t100000\n"+ "yellow\t70000\n",readFile(testOutputDir.getAbsolutePath() + "/" + (useNewReducer ? "part-r-" : "part-")+ "00000"));
    }
  finally {
      FileUtils.deleteDirectory(testOutputDir);
    }
  }
}
