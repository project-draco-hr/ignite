{
  GridGgfsPath inDir=new GridGgfsPath(PATH_INPUT);
  ggfs.mkdirs(inDir);
  GridGgfsPath inFile=new GridGgfsPath(inDir,GridHadoopWordCount2.class.getSimpleName() + "-input");
  generateTestFile(inFile.toString(),"red",100000,"blue",200000,"green",150000,"yellow",70000);
  for (int i=0; i < 16; i++) {
    ggfs.delete(new GridGgfsPath(PATH_OUTPUT),true);
    boolean useNewMapper=(i & 1) == 0;
    boolean useNewCombiner=(i & 2) == 0;
    boolean useNewReducer=(i & 4) == 0;
    boolean useCustomSerializer=(i & 8) == 0;
    JobConf jobConf=new JobConf();
    if (useCustomSerializer)     jobConf.set(CommonConfigurationKeys.IO_SERIALIZATIONS_KEY,CustomSerialization.class.getName());
    jobConf.setInt(FileInputFormat.SPLIT_MAXSIZE,65000);
    jobConf.setInt("fs.local.block.size",65000);
    setupFileSytems(jobConf);
    GridHadoopWordCount1.setTasksClasses(jobConf,!useNewMapper,!useNewCombiner,!useNewReducer);
    Job job=Job.getInstance(jobConf);
    GridHadoopWordCount2.setTasksClasses(job,useNewMapper,useNewCombiner,useNewReducer);
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(IntWritable.class);
    FileInputFormat.setInputPaths(job,new Path(ggfsScheme() + inFile.toString()));
    FileOutputFormat.setOutputPath(job,new Path(ggfsScheme() + PATH_OUTPUT));
    job.setJarByClass(GridHadoopWordCount2.class);
    GridFuture<?> fut=grid(0).hadoop().submit(new GridHadoopJobId(UUID.randomUUID(),1),new GridHadoopDefaultJobInfo(job.getConfiguration()));
    fut.get();
    assertEquals("Use new mapper = " + useNewMapper + ", combiner = "+ useNewCombiner+ "reducer = "+ useNewReducer,"blue\t200000\n" + "green\t150000\n" + "red\t100000\n"+ "yellow\t70000\n",readAndSortFile(PATH_OUTPUT + "/" + (useNewReducer ? "part-r-" : "part-")+ "00000"));
  }
}
