{
  execStartTs=U.currentTimeMillis();
  Throwable err=null;
  HadoopTaskState state=HadoopTaskState.COMPLETED;
  HadoopPerformanceCounter perfCntr=null;
  try {
    ctx=job.getTaskContext(info);
    perfCntr=HadoopPerformanceCounter.getCounter(ctx.counters(),nodeId);
    perfCntr.onTaskSubmit(info,submitTs);
    perfCntr.onTaskPrepare(info,execStartTs);
    ctx.prepareTaskEnvironment();
    runTask(perfCntr);
    if (info.type() == MAP && job.info().hasCombiner()) {
      ctx.taskInfo(new HadoopTaskInfo(COMBINE,info.jobId(),info.taskNumber(),info.attempt(),null));
      try {
        runTask(perfCntr);
      }
  finally {
        ctx.taskInfo(info);
      }
    }
  }
 catch (  HadoopTaskCancelledException ignored) {
    state=HadoopTaskState.CANCELED;
  }
catch (  Throwable e) {
    state=HadoopTaskState.FAILED;
    err=e;
    U.error(log,"Task execution failed.",e);
  }
 finally {
    execEndTs=U.currentTimeMillis();
    if (perfCntr != null)     perfCntr.onTaskFinish(info,execEndTs);
    onTaskFinished(new HadoopTaskStatus(state,err,ctx == null ? null : ctx.counters()));
    if (combinerInput != null)     combinerInput.close();
    if (ctx != null)     ctx.cleanupTaskEnvironment();
  }
  return null;
}
